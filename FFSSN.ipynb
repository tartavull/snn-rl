{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.3em;text-decoration:underline;font-weight:bold'><center>Reinforcement Learning Using Spiking Neural Networks</center></div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Latest Results</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>\n",
    "The chart below is based trained neuron responses from each letter of the alphabet being presented 3 times in repetition then having the next letter presented.  Neurons, as represented as y-axis values, having points on the graph specific to only 1 letter (3 points in a row) shows the model having learned well and otherwise shows an area learning can improve.  Points represent neurons responding (being activated) through firing spikes.  Highest performance has been measured as 97.3% overall accuracy (TP+TN/TP+FP+TN+FN) and 61.5% precision (TP/TP+FP) to input.  Work is undergoing now to auto-optimize parameters to increase performance even further.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/optSim.ipynb'>Parameter Optimization</a>\n",
    "<center><br>$\\small\\hspace{3pt}Realistic\\hspace{1pt}Neuron\\hspace{1pt}Property\\hspace{1pt}Modeling\\hspace{1pt}Key\\hspace{1pt}Features: Spike\\hspace{1pt}Timing\\hspace{1pt}Dependent\\hspace{1pt}Plasticity*Active$<br>$\\small Dendrites*Direct\\hspace{1pt}to\\hspace{1pt}Soma\\hspace{1pt}Signaling*Lateral\\hspace{1pt}Inhibition * Learning\\hspace{1pt}Rate * Neurosci.\\hspace{1pt}Toolkit$</center>\n",
    "<br><br><center>26 Char Test Results with Spike Occurences of Neurons<img src='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTest.jpg'><br><br></center>\n",
    "\n",
    "Letters used as stimuli were represented as images with 15 pixels (3x5) and presented sequentially for 100ms at a time 3 times in a row.  Spikes occur when a neuron's voltage reaches 10mv.  Once a spike occurs the voltages of the other neurons are inhibited which is visible as their large drops on the plot.  The inhibition and excitation is greater than wanted (-65mv at some points) but work is being done to scale that better.  An area undergoing active work is improving the discrimination of characters close in pixel presentation to each other, for example 'B' and 'D' have the same neuron in the video below.  <a target=\"_blank\" href=\"http://nmsutton.heroku.com\">Nate</a> will work further on the performance after his current work on joining a lab or other position, <a target=\"_blank\" href='https://www.linkedin.com/in/tartavull'>Ignacio</a> as well if he can get the chance but we welcome contributions to this open source project.  The video below shows a close up view of voltage response measurement of four neurons trained to respond to example letters (A, B, C, and D).  We are trying to improve the simulations eyesight like a human would have trying to read a eye chart!  Overall the neurons specialized reasonably well to the input but there is room for greater performance as well. \n",
    "\n",
    "<br><br>Intro material: <a target=\"_blank\" href='https://github.com/tartavull/snn-rl/blob/master/README.md'>ReadMe</a> and <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/introduction.ipynb'>Intro</a>.  Authors: <a target=\"_blank\" href=\"http://nmsutton.heroku.com\">Nate</a> and <a target=\"_blank\" href='https://www.linkedin.com/in/tartavull'>Ignacio</a>.  Our <a target=\"_blank\" href='https://github.com/tartavull/snn-rl'>code</a>.\n",
    "<br>Based on: <a target=\"_blank\" href='http://www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>Character Recognition using Spiking Neural Networks</a>.\n",
    "\n",
    "<br><br>You may ask, how did the neurons get trained?  Let me explain!<br><br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><div style=\"font-size:1.5em\"><iframe width=\"80%\" height=\"700\" src=\"http://www.youtube.com/embed/DUZldskw51A\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/notebooks/furtherFormulas/3dAnimScatterPlotHdf5.ipynb\">Animation</a> <a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/testVoltage.mp4\">Video Download</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML;HTML('<center><div style=\"font-size:1.5em\"><iframe width=\"80%\" height=\"700\" src=\"http://www.youtube.com/embed/DUZldskw51A\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/notebooks/furtherFormulas/3dAnimScatterPlotHdf5.ipynb\">Animation</a> <a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/testVoltage.mp4\">Video Download</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Training</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Values are trained for the model by displaying each character after each other for one spike interval (100ms).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Time and Refractory Period</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Time is initally incremented and refractory period status variables are processed.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/timeAndRefracCalcs.ipynb#timePeriodAndRefractoryCalcs'>Time&Refrac</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>First layer and Dirac function</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Spikes from the first layer are represented as presynaptic spike times across a 11 epoch (30000ms) timeframe.  They are coded into a variable that is has lookups done in it to refer back to when spikes have occured.  Spikes found trigger a dirac function which is one of the cofactors in the dendrite and somaDirect equations which build their signal that leads to the soma potential.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#diracCalc'>Dirac Function</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Weight</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The weights are set at random initially but at a range of values that allows dirac activated dendrite and someDirect signals to \n",
    "combine to create postsynaptic spikes (from output neurons) from the beginning or soon enough afterward.  After initial spiking has occured the reinforcement learning comes into effect where character input that causes a presynaptic spike and post synapic spike after that causes a weight increase.  The increase is defined by the weighting equations.  <a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>weight change</a>, \n",
    "<a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>returnDeltaW</a>, \n",
    "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#returnNewW'>returnNewW</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><br><div style=\"font-size:1.5em\">Weights of All Neurons During the Training Process with 4 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/-ae2j13XQBU\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartAnim.ipynb#returnNewW\">3dBarChartAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><br><div style=\"font-size:1.5em\">Weights of All Neurons During the Training Process with 4 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/-ae2j13XQBU\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartAnim.ipynb#returnNewW\">3dBarChartAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The below video shows the final weights produced after training 26 neurons with the full alphabet for 30000ms total time.  Each neuron is intended to have weights specialized in accordance with only one distinct character.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><br><div style=\"font-size:1.5em\">Weights of All Neurons After Training with 26 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/8R_BrMk8VYM\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartRotatingAnim.ipynb#returnNewW\">3dBarChartRotatingAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><br><div style=\"font-size:1.5em\">Weights of All Neurons After Training with 26 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/8R_BrMk8VYM\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartRotatingAnim.ipynb#returnNewW\">3dBarChartRotatingAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'><center>Weights of the Neurons After Training with 4 Characters:<img src=\"http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/weights.jpeg\"><a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartGenerator.ipynb#returnNewW'>3dBarChartGenerator</a></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Tau</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The learning rate for the dendrite, tau, is dependednt on the results of the weight calculation.  It creates faster learning when a stronger weight is present.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#tauDCalc'>Tau</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Resistance</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Resistance is dependent on tau and assists with allowing the voltage to reach a spiking level even with lower weights and tau.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#resistanceCalc'>Resistance</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><div style=\"font-size:1.5em\">Relationship Between Weight, Tau, and Resistance:<br><br></div><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/f9QN9Q1FqPY\" loop=1 allowfullscreen></iframe><br><div style=\"font-size:1.5em\"><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarWTauRAnim.ipynb#returnNewW\">3dBarWTauRAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><div style=\"font-size:1.5em\">Relationship Between Weight, Tau, and Resistance:<br><br></div><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/f9QN9Q1FqPY\" loop=1 allowfullscreen></iframe><br><div style=\"font-size:1.5em\"><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarWTauRAnim.ipynb#returnNewW\">3dBarWTauRAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Dendrite Input</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>This is one receptor of presynaptic input that translates signals into other values that are passed to the soma.  \n",
    "The equation below specifies how.  Cofactors in the equation are devided by their units to normalize the values.  <br>Dendrite equ (equ. 1) in the <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a> is $\\tau_d^i*d_(I^i_d(t))/dt=-I_d^i(t)+R_d^i*w^i*\\delta (t-t_f^i)$ <br>Equ in Brian2 (open source prog):  dv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt  \n",
    "<a target=\"_blank\" href='#mainCode'>Dendrite Equations</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Direct Input</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Another receptor of presynaptic input.  <br>SomaDirect equ (equ. 4) in the <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a> is $\\tau_s*d_(I_s(t))/dt=-I_s(t)\\sum_{i}w^i*\\delta (t-t_f^i)$ <br>In Brian2:  dv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt  \n",
    "<a target=\"_blank\" href='#mainCode'>SomaDirect Equations</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Lateral Inhibition</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Due to competition amongst neurons inhibition signals are sent from one to another when they receive input.  A winner-take-all type of implementation was created where inhibition is auto tuned, the neuron with the greatest soma membrane potential change is the only one allowed to create a post-synaptic spike.  The membrane potential is scaled based on the inhibition.\n",
    "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/lateralInhibition.ipynb#lateralInhibition'>Lateral Inhibition</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Membrane Potential Charge (Um)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Um (equ. 5) in <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a>: $\\tau_m*d_(U_m(t))/dt=-U_m(t)+R_m(I_d(t)+I_s(t))$ <br>\n",
    "Um prior to lat. inh.: dprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
    "<br>Um after lat. inh.: dv/dt = v/(1*second): volt\n",
    "<a target=\"_blank\" href='#mainCode'><br>Soma Membrane Potential Equations</a><br><br><center>Spikes of Neurons During Training<img src=\"http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/trainingSpikes.jpg\">The dots represent a spike fired for an input character stimulus.  Notice how over time the neurons specialize in the way they designate themselves for only one character.  That is the reinforcement learning causing the neurons to specilize and in this example it takes toward 10000ms for that.  In some training simulations they do not all correctly become designated to one character.\n",
    "<br><br>\n",
    "Below the spikes generated during training with the full alphabet are shown.  Greater specialization toward the end shows the degree of effectiveness in the training.</center>\n",
    "\n",
    "<br><br><center>26 Char Training with Neuron Spikes<a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTrain.jpg'><img src='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTrain1024Width.jpg'>Click to enlarge</a></center>\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Check for resets</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>After each spike interval has passed some logic is included to reset values upon spike occurences.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#mainCode'>Resets</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Testing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>The weights and subsequently tau and resistance generated are used as the trained model values and tests are run to evaluate the performance.  Input characters are presented three times in a row for three spike intervals (300ms total).  Observed spikes fired and not are compared to expected values and performance is reported.\n",
    "<a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#intitializeTrainedModelParameters'>intitializeTrainedModelParameters</a>, <a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#evaluateClassifierPerf'>evaluateClassifierPerf</a>, <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/testingProcesses.ipynb#OutputEvaluationResults'>OutputEvaluationResults</a><br><br><center>Further descriptions of the simulation results are <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/analysisFurtherFormulas.ipynb'>here</a></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Main code for the simulation:<a id='mainCode'></a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from furtherFormulas.architecture_further_formulas import *\n",
    "from furtherFormulas.cofactorCalculations import *\n",
    "from furtherFormulas.timeAndRefracCalcs import *\n",
    "from furtherFormulas.outputPrinting import *\n",
    "from furtherFormulas.testingProcesses import *\n",
    "from furtherFormulas.lateralInhibition import *\n",
    "\n",
    "timeAndRefrac = timeAndRefrac\t\n",
    "\n",
    "class gupta_paper:\n",
    "\n",
    "\tneuralnet = Network()\n",
    "\tdictionary = dictionary()\n",
    "\tspiketimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, testingSpikesPerChar, testingEpochs)\n",
    "\ttrainingSpikeTimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, trainingSpikesPerChar, trainingEpochs)\n",
    "\tLIK = SpikeGeneratorGroup(N=15, indices=spiketimes[:,0], times=spiketimes[:,1]*ms)\n",
    "\t# W = W and other lines below are to avoid an odd 'reference before assignment' error\n",
    "\tW = W\n",
    "\ttauM = tauM\n",
    "\tneuronIndex = neuronIndex\n",
    "\tgeneralClockDt = generalClockDt\n",
    "\trunTime = runTime\n",
    "\trunTimeScaling = runTimeScaling\n",
    "\tevaluateClassifier = evaluateClassifier\n",
    "\taccelerateTraining = accelerateTraining\n",
    "\tdiracScaling = diracScaling\n",
    "\tsomaDirectScaling = somaDirectScaling\n",
    "\tnegativeWeightReinforcement = negativeWeightReinforcement\n",
    "\tpositiveWeightReinforcement = positiveWeightReinforcement\n",
    "\ttimeAndRefrac = timeAndRefrac\t\n",
    "\ttestRun = testRun\n",
    "\tlatInhibSettings = latInhibSettings\n",
    "\n",
    "\tprint 'initial Weights\\n',W\n",
    "\n",
    "\tdef run_model(self):\n",
    "\t\tneuralnet = self.neuralnet\n",
    "\t\tdictionary = self.dictionary\n",
    "\t\teqs = Equations('''\n",
    "\t\t\tdv/dt = v/(1*second): volt\n",
    "\t\t\tdprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
    "\t\t\tRm = 80*mV : volt\n",
    "\t\t\ttauM = 30*ms : second\n",
    "\t        V : volt\n",
    "\t        DendI : volt\n",
    "\t        SynI : volt\n",
    "\t        v2 : volt\t\n",
    "\t\t\tUmSpikeFired : volt\t\n",
    "\t\t\tbeginRefrac : volt\n",
    "\t\t    ''')\t\t\t\n",
    "\n",
    "\t\tdendriteEqs = Equations('''\n",
    "\t\t\tdv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt\n",
    "\t\t\tV : volt\n",
    "\t        r : volt\n",
    "\t        w : volt\n",
    "\t        dirac : volt\n",
    "\t        tau : second\n",
    "\t        v2: volt\n",
    "\t\t\t''')\n",
    "\n",
    "\t\tdirectToSomaEqs = Equations('''\n",
    "\t\t\tdv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt\n",
    "\t\t\ttauS = 2*ms : second\n",
    "\t\t\tV : volt\n",
    "\t\t\tsummedWandDirac : volt\n",
    "\t\t\tv2 : volt\n",
    "\t\t\tspikeFired : boolean\n",
    "\t\t\t''')\t\t\n",
    "\n",
    "\t\tclass ADDSNeuronModel(NeuronGroup, gupta_paper): \n",
    "\t\t\tneuronIndex = self.neuronIndex\n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\t\t\tdef __init__(self, params):\n",
    "\t\t\t\tNeuronGroup.__init__(self, N=dictionaryLongitude, model=eqs,threshold='v>10*mV', reset='v=-0.002 * mV; dv=0; v2=10*mV;UmSpikeFired=1*mV;beginRefrac=1*mV;inhibitionVoltage=prelimV',refractory=8*ms,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tneuronIndex = self.neuronIndex\n",
    "\t\t\t\t\ttimeAndRefrac.spikeIntervalCounter = (floor(timeAndRefrac.time/timeAndRefrac.spikeIntervalUnformatted) * timeAndRefrac.spikeIntervalUnformatted)*10\n",
    "\n",
    "\t\t\t\t\tdef dendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive):\n",
    "\t\t\t\t\t\t# Below sequentially Dirac, Tau, then Resistance are calculated every end of a spike-time interval.\n",
    "\t\t\t\t\t\t# The resulting Dend I is added to the Um calc for the ADDS soma.\n",
    "\t\t\t\t\t\ttimeAndRefrac = self.timeAndRefrac\n",
    "\n",
    "\t\t\t\t\t\t# Dirac\n",
    "\t\t\t\t\t\tdendObj[neuronIndex].dirac = diracCalc(dendObj, neuronIndex, spiketimes, timeAndRefrac.time, timeAndRefrac.lastSpikeInterval)\n",
    "\n",
    "\t\t\t\t\t\t# Initialize weights\n",
    "\t\t\t\t\t\tif (evaluationActive==False and timeAndRefrac.time == 0.000):\n",
    "\t\t\t\t\t\t\tdend[neuronIndex].w = W[neuronIndex]*volt\n",
    "\n",
    "\t\t\t\t\t\tif (evaluationActive==False and timeAndRefrac.refractoryPointSwitch==True):\n",
    "\t\t\t\t\t\t\t# Only change weights of neuron fired\n",
    "\t\t\t\t\t\t\tif ADDSObj.UmSpikeFired[neuronIndex] == 1*mV:\n",
    "\t\t\t\t\t\t\t\t# Weights\n",
    "\t\t\t\t\t\t\t\tWeightChangeCalculation(neuronIndex, spiketimes, timeAndRefrac.time, self.negativeWeightReinforcement, self.positiveWeightReinforcement, M, dendObj)\t\n",
    "\t\t\t\t\t\t\t# Tau\n",
    "\t\t\t\t\t\t\ttauDCalc(neuronIndex, dendObj)\n",
    "\t\t\t\t\t\t\t# Resistance\n",
    "\t\t\t\t\t\t\tresistanceCalc(neuronIndex, dendObj, self.tauM)\n",
    "\n",
    "\t\t\t\t\t\t# TODO: check do I need additional loop below?\n",
    "\t\t\t\t\t\tfor indexOfDend in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\tADDSObj.DendI[indexOfDend] = sum(dendObj[indexOfDend].v[:])*dendCalcScaling\n",
    "\n",
    "\t\t\t\t\t\t#print 'ADDSObj.t',ADDSObj.t,'ADDSObj.DendI',ADDSObj.DendI,'neuronIndex',neuronIndex,'dendObj[neuronIndex].dirac',dendObj[neuronIndex].dirac,'dendObj[neuronIndex].tau',dendObj[neuronIndex].tau,'dendObj[neuronIndex].w',dendObj[neuronIndex].w,'dendObj[neuronIndex].r',dendObj[neuronIndex].r\n",
    "\n",
    "\t\t\t\t\tdef somaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj):\n",
    "\t\t\t\t\t\tdotProductWandDirac =  sum(w*d for w,d in zip(dendObj[neuronIndex].w[:], dendObj[neuronIndex].dirac[:]))\n",
    "\t\t\t\t\t\t#somaDirectObj.summedWandDirac[neuronIndex] = ((dotProductWandDirac*volt)/(volt**2))*self.somaDirectScaling\n",
    "\t\t\t\t\t\tsomaDirect.summedWandDirac[neuronIndex] = ((dotProductWandDirac*volt)/(volt**2))*self.somaDirectScaling\n",
    "\n",
    "\t\t\t\t\t\tfor neuronNumber in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t#ADDSObj.SynI[neuronNumber] = somaDirectObj.v[neuronNumber]\t\t\n",
    "\t\t\t\t\t\t\tADDS.SynI[neuronNumber] = somaDirect.v[neuronNumber]\t\t\n",
    "\n",
    "\t\t\t\t\t\t#print 'ADDSObj.t',ADDSObj.t,'ADDSObj.SynI',ADDSObj.SynI,'neuronIndex',neuronIndex,'somaDirectObj.summedWandDirac',somaDirectObj.summedWandDirac[neuronIndex],'dendObj[neuronIndex].w',dendObj[neuronIndex].w,'dendObj[neuronIndex].dirac',dendObj[neuronIndex].dirac\n",
    "\n",
    "\t\t\t\t\tdef mainSimulationCalcs(ADDSObj, dendObj, somaDirectObj, spiketimes, evaluationActive):\n",
    "\t\t\t\t\t\t# dend then somaDirect calcs are done which are then used to set lat inhib.\n",
    "\t\t\t\t\t\t# Soma Um calcs are done automatically using equations entered for brian\n",
    "\t\t\t\t\t\t# once dend and somaDirect are updated\n",
    "\t\t\t\t\t\tpreTNorm = self.timeAndRefrac.time\n",
    "\t\t\t\t\t\ttNorm = preTNorm - (floor((preTNorm/.001)*.01) * .1)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tself.timeAndRefrac = timePeriodAndRefractoryCalcs(self.timeAndRefrac)\n",
    "\n",
    "\t\t\t\t\t\tif (evaluationActive==True) and (timeAndRefrac.time == 0.000 or timeAndRefrac.time == 0.001):\n",
    "\t\t\t\t\t\t\tinitializeTrainedModelParameters(dendObj)\n",
    "\n",
    "\t\t\t\t\t\t# Option to accelerate computations for training\n",
    "\t\t\t\t\t\tif self.accelerateTraining == False or (evaluationActive == False and (tNorm <= .005 or tNorm >= .096)):\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tif self.accelerateTraining == True and (tNorm >= .096 and tNorm < .099):\n",
    "\t\t\t\t\t\t\t\tfor i in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.DendI[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.SynI[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.prelimV[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.v[i]=0*mV\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\tfor i2 in range(len(dend[0].v)):\n",
    "\t\t\t\t\t\t\t\t\t\tdendObj[i].v[i2] = 0*mV\n",
    "\t\t\t\t\t\t\t\t\tsomaDirectObj.v[i] = 0*mV\t\t\n",
    "\n",
    "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\tdendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive)\n",
    "\n",
    "\t\t\t\t\t\t\t\tsomaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj)\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\tADDSObj.v, self.latInhibSettings = lateralInhibition(ADDSObj, self.timeAndRefrac, self.latInhibSettings)\n",
    "\n",
    "\t\t\t\t\t\t\t#if ADDSObj.t > 100*ms:\n",
    "\t\t\t\t\t\t\t\t#for i in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\t#\tsomaDirectObj.summedWandDirac[i] += 20*mV\n",
    "\t\t\t\t\t\t\t\t#print 'ADDS.t',ADDS.t,'ADDS.SynI',ADDS.SynI,'ADDS.DendI',ADDS.DendI,'ADDSObj.v',ADDSObj.v,'somaDirectObj.summedWandDirac',somaDirectObj.summedWandDirac,'somaDirectObj.v',somaDirectObj.v\n",
    "\n",
    "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude): \n",
    "\t\t\t\t\t\t\t\tADDSObj.v2, somaDirectObj.v2, self.timeAndRefrac = checkForResets(neuronIndex, ADDSObj, dendObj, somaDirectObj, self.timeAndRefrac)\n",
    "\n",
    "\t\t\t\t\t\t\tADDSObj.UmSpikeFired, self.testRun = evaluateClassifierPerf2(ADDSObj, self.testRun)\n",
    "\n",
    "\t\t\t\t\t\t'''roundedSecondsTime = math.floor(Decimal(format((ADDSObj.t), '.1f'))/Decimal(format((1.0*second), '.1f')))\n",
    "\t\t\t\t\t\tif printWeights < roundedSecondsTime:\n",
    "\t\t\t\t\t\t\tself.printWeights = roundedSecondsTime; printWeights(dendObj);'''\n",
    "\t\t\t\t\tif self.evaluateClassifier == False:\n",
    "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.trainingSpikeTimes, False)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.spiketimes, True)\t\t\t\t\t\n",
    "\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\t\t\n",
    "\n",
    "\t\tclass DendriteNeuronModel(NeuronGroup):\n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\t\t\tdef __init__(self, params): \n",
    "\t\t\t\tNeuronGroup.__init__(self, N=15, model=dendriteEqs,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tplaceHolderForLaterContent = True\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\n",
    "\n",
    "\t\tclass SomaDirectNeuronModel(NeuronGroup): \n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\t\t\tdef __init__(self, params): \n",
    "\t\t\t\tNeuronGroup.__init__(self, N=dictionaryLongitude, model=directToSomaEqs,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tplaceHolderForLaterContent = True\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\n",
    "\n",
    "\t\tdend = [None]*dictionaryLongitude\n",
    "\t\tweightMonitors = [None]*dictionaryLongitude\n",
    "\t\tfor firstLayerIndex in range(dictionaryLongitude):\n",
    "\t\t\tdend[firstLayerIndex] = DendriteNeuronModel(15)\t\n",
    "\t\t\tweightMonitors[firstLayerIndex] = StateMonitor(dend[firstLayerIndex], 'w', record=True)\n",
    "\t\t\tneuralnet.add(dend[firstLayerIndex])\n",
    "\t\t\tneuralnet.add(weightMonitors[firstLayerIndex])\n",
    "\t\tsomaDirect = SomaDirectNeuronModel(dictionaryLongitude)\n",
    "\t\tneuralnet.add(somaDirect)\n",
    "\t\tADDS = ADDSNeuronModel(self)\t\t\t\n",
    "\t\tM = SpikeMonitor(ADDS)\n",
    "\t\tMv = StateMonitor(ADDS, 'V', record=True)\n",
    "\t\tUmM = StateMonitor(ADDS, 'v2', record=True)\n",
    "\t\tself.M = M # for ipython compatibility\n",
    "\t\tself.UmM = UmM \n",
    "\t\tself.weightMonitors = weightMonitors\n",
    "\n",
    "\t\tneuralnet.add(ADDS)\n",
    "\t\tneuralnet.add(M)\n",
    "\t\tneuralnet.add(UmM)\n",
    "\t\tself.runTime *= self.runTimeScaling # scaling factor\n",
    "\t\tneuralnet.run(self.runTime,report='text')\n",
    "\n",
    "\t\tOutputEvaluationResults(dend, self.testRun)\n",
    "\n",
    "\t\tneuronToPlot = 1\n",
    "\t\tcolors = ['r']*1+['g']*1+['b']*1+['y']*1\n",
    "\t\tcolors = ['blue', 'green', 'magenta', 'cyan']\n",
    "\t\tsubplot(211)\n",
    "\t\tplot(M.t/ms, M.i, '.')\n",
    "\t\tlegend(['A','B','C','D'], loc='upper left')\t\t\t\n",
    "\t\tsubplot(212)\n",
    "\t\tplot(UmM.t, UmM.v2.T/mV)\t\n",
    "\t\txlabel('Time (ms)')\n",
    "\t\tylabel('Membrane Potential (mV)')\n",
    "\t\tshow()\t\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.run_model()\n",
    "\n",
    "def main():\n",
    "\trun_gupta_paper = gupta_paper()\n",
    "\n",
    "if  __name__ =='__main__':main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
