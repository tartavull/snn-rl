{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.3em;font-weight:bold'><center>Reinforcement Learning Using Spiking Neural Networks</center></div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Latest Results</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>\n",
    "The chart below is based on trained neuron responses from each letter of the alphabet being presented 3 times in repetition then having the next letter presented. Neurons, as represented as y-axis values, having points on the graph specific to only 1 letter (3 points in a row) shows the model has learned well, and otherwise shows an area learning can improve. Points represent neurons responding (being activated) through firing spikes. Work has been done to auto-optimize parameters with 90 runs with different parameter settings. Parameters from amongst the top scoring ones selected for further testing are in this training and testing <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/notebooks/trainTestSim.ipynb'>script</a>.  Also, <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/notebooks/optSim.ipynb'>parameter auto-optimization</a> code.  Average performance using the optimized parameters for 8 runs has been measured as 97.3% overall accuracy (TP+TN/TP+FP+TN+FN) and 62.8% (high 78%, low 46%) precision (TP/TP+FP).\n",
    "<center><br>$\\small\\hspace{3pt}Realistic\\hspace{1pt}Neuron\\hspace{1pt}Property\\hspace{1pt}Modeling\\hspace{1pt}Key\\hspace{1pt}Features: Spike\\hspace{1pt}Timing\\hspace{1pt}Dependent\\hspace{1pt}Plasticity*Active$<br>$\\small Dendrites*Direct\\hspace{1pt}to\\hspace{1pt}Soma\\hspace{1pt}Signaling*Lateral\\hspace{1pt}Inhibition * Learning\\hspace{1pt}Rate * Neurosci.\\hspace{1pt}Toolkit$</center>\n",
    "<br><br><center>26 Char Test Results with Spike Occurrences of Neurons<a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTest.jpg'><img src='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTest1024Width.jpg'></a><br><br></center>\n",
    "\n",
    "Letters used as stimuli were represented as images with 15 pixels (3x5) and presented sequentially for 100ms at a time 3 times in a row. Spikes occur when a neuron's voltage reaches 10mv. Once a spike occurs the voltages of the other neurons are inhibited which is visible as their large drops on the plot. The inhibition and excitation are greater than wanted (-65mv at some points) but work is being done to scale that better. An area undergoing active work is improving the discrimination of characters close in pixel presentation to each other, for example, 'B' and 'D' have the same neuron in the video below.  <a target=\"_blank\" href=\"http://nmsutton.herokuapp.com\">Nate</a> will work further on the performance after his current work on joining a lab or other position, <a target=\"_blank\" href='https://www.linkedin.com/in/tartavull'>Ignacio</a> as well if he can get the chance but we welcome contributions to this open source project. The video below shows a close-up view of voltage response measurement of four neurons trained to respond to example letters (A, B, C, and D). We are trying to improve the simulation's eyesight in the way a human would trying to read an eye chart! Overall the neurons specialized reasonably well to the input, but there is room for greater performance as well.\n",
    "\n",
    "<br><br>Intro material: <a target=\"_blank\" href='https://github.com/tartavull/snn-rl/blob/master/README.md'>ReadMe</a> and <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/notebooks/introduction.ipynb'>Intro</a>.  Authors: <a target=\"_blank\" href=\"http://nmsutton.herokuapp.com\">Nate</a> and <a target=\"_blank\" href='https://www.linkedin.com/in/tartavull'>Ignacio</a>.  Our <a target=\"_blank\" href='https://github.com/tartavull/snn-rl'>code</a>.\n",
    "<br>Based on: <a target=\"_blank\" href='http://www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>Character Recognition using Spiking Neural Networks</a>.\n",
    "\n",
    "<br><br>You may ask, how did the neurons get trained?  Let me explain!<br><br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><div style=\"font-size:1.5em\"><iframe width=\"80%\" height=\"700\" src=\"http://www.youtube.com/embed/DUZldskw51A\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/notebooks/furtherFormulas/3dAnimScatterPlotHdf5.ipynb\">Animation</a> <a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/testVoltage.mp4\">Video Download</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML;HTML('<center><div style=\"font-size:1.5em\"><iframe width=\"80%\" height=\"700\" src=\"http://www.youtube.com/embed/DUZldskw51A\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/notebooks/furtherFormulas/3dAnimScatterPlotHdf5.ipynb\">Animation</a> <a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/testVoltage.mp4\">Video Download</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2em;text-decoration:underline;font-weight:bold'>Training</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Values are trained for the model by displaying each character after each other for one spike interval (100ms).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Time and Refractory Period</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Time is initially incremented and refractory period status variables are processed.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/timeAndRefracCalcs.ipynb#timePeriodAndRefractoryCalcs'>Time&Refrac</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>First layer and Dirac function</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Spikes from the first layer are represented as presynaptic spike times across an 11 epoch (30000ms) timeframe. They are coded into a variable that has lookups done in it to refer back to when spikes have occurred. Spikes that are found cause the triggering of a dirac function which is one of the cofactors in the dendrite and somaDirect equations, which build their signal that leads to the soma potential.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#diracCalc'>Dirac Function</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Weight</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The weights are set at random initially but at a range of values that allows dirac activated dendrite and somaDirect signals to combine to create postsynaptic spikes (from output neurons) from the beginning or soon enough afterward. After initial spiking has occurred the reinforcement learning comes into effect where character input that causes a presynaptic spike and post synaptic spike after that causes a weight increase. The increase is defined by the weighting equations.  <a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>weight change</a>, \n",
    "<a href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb'>returnDeltaW</a>, \n",
    "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#returnNewW'>returnNewW</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><br><div style=\"font-size:1.5em\">Weights of All Neurons During the Training Process with 4 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/-ae2j13XQBU\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartAnim.ipynb#returnNewW\">3dBarChartAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><br><div style=\"font-size:1.5em\">Weights of All Neurons During the Training Process with 4 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/-ae2j13XQBU\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartAnim.ipynb#returnNewW\">3dBarChartAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The below video shows the final weights produced after training 26 neurons with the full alphabet for 30000ms total time.  Each neuron is intended to have weights specialized in accordance with only one distinct character.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><br><div style=\"font-size:1.5em\">Weights of All Neurons After Training with 26 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/8R_BrMk8VYM\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartRotatingAnim.ipynb#returnNewW\">3dBarChartRotatingAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><br><div style=\"font-size:1.5em\">Weights of All Neurons After Training with 26 Characters<br><br><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/8R_BrMk8VYM\" loop=1 allowfullscreen></iframe><br><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartRotatingAnim.ipynb#returnNewW\">3dBarChartRotatingAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'><center>Weights of the Neurons After Training with 4 Characters:<img src=\"http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/weights.jpeg\"><a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarChartGenerator.ipynb#returnNewW'>3dBarChartGenerator</a></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Tau</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>The learning rate for the dendrite, tau, is dependent on the results of the weight calculation. It creates faster learning when a stronger weight is present.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#tauDCalc'>Tau</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Resistance</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>Resistance is dependent on tau and assists with allowing the voltage to reach a spiking level even with lower weights and tau.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#resistanceCalc'>Resistance</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><div style=\"font-size:1.5em\">Relationship Between Weight, Tau, and Resistance:<br><br></div><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/f9QN9Q1FqPY\" loop=1 allowfullscreen></iframe><br><div style=\"font-size:1.5em\"><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarWTauRAnim.ipynb#returnNewW\">3dBarWTauRAnim</a></div></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML;HTML('<center><div style=\"font-size:1.5em\">Relationship Between Weight, Tau, and Resistance:<br><br></div><iframe width=\"60%\" height=\"500\" src=\"http://www.youtube.com/embed/f9QN9Q1FqPY\" loop=1 allowfullscreen></iframe><br><div style=\"font-size:1.5em\"><a target=\"_blank\" href=\"/github/tartavull/snn-rl/blob/master/furtherFormulas/3dBarWTauRAnim.ipynb#returnNewW\">3dBarWTauRAnim</a></div></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Dendrite Input</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em'>This is one receptor of presynaptic input that translates signals into other values that are passed to the soma. The equation below specifies how. Cofactors in the equation are divided by their units to normalize the values.  <br>Dendrite equ (equ. 1) in the <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a> is $\\tau_d^i*d_(I^i_d(t))/dt=-I_d^i(t)+R_d^i*w^i*\\delta (t-t_f^i)$ <br>Equ in Brian2 (open source prog):  dv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt  \n",
    "<a target=\"_blank\" href='#mainCode'>Dendrite Equations</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Direct Input</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Another receptor of presynaptic input.  <br>SomaDirect equ (equ. 4) in the <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a> is $\\tau_s*d_(I_s(t))/dt=-I_s(t)\\sum_{i}w^i*\\delta (t-t_f^i)$ <br>In Brian2:  dv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt  \n",
    "<a target=\"_blank\" href='#mainCode'>SomaDirect Equations</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Lateral Inhibition</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Due to competition amongst neurons inhibition signals are sent from one to another when they receive input. A winner-take-all type of implementation was created where inhibition is auto-tuned, the neuron with the greatest soma membrane potential change is the only one allowed to create a post-synaptic spike. The membrane potential is scaled based on the inhibition.\n",
    "<a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/lateralInhibition.ipynb#lateralInhibition'>Lateral Inhibition</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Soma Membrane Potential Charge (Um)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>Um (equ. 5) in <a target=\"_blank\" href='www.personal.psu.edu/lnl/papers/Gupta_Long_2007.pdf'>paper</a>: $\\tau_m*d_(U_m(t))/dt=-U_m(t)+R_m(I_d(t)+I_s(t))$ <br>\n",
    "Um prior to lat. inh.: dprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
    "<br>Um after lat. inh.: dv/dt = v/(1*second): volt\n",
    "<a target=\"_blank\" href='#mainCode'><br>Soma Membrane Potential Equations</a><br><br><center>Spikes of Neurons During Training<img src=\"http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/trainingSpikes.jpg\">The dots represent a spike fired for an input character stimulus. Notice how over time the neurons specialize in the way they designate themselves for only one character. That is the reinforcement learning causing the neurons to specialize, and in this example, it takes toward 10000ms for that. In some training simulations they do not all correctly become designated to one character.\n",
    "<br><br>\n",
    "Below the spikes generated during training with the full alphabet are shown.  Greater specialization toward the end shows the degree of effectiveness in the training.</center>\n",
    "\n",
    "<br><br><center>26 Char Training with Neuron Spikes<a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTrain.jpg'><img src='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/img/26charTrain1024Width.jpg'>Click to enlarge</a></center>\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.7em;text-decoration:underline;font-weight:bold'>Check for resets</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>After each spike interval has passed, some logic is included to reset values upon spike occurrences.  <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/cofactorCalculations.ipynb#mainCode'>Resets</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Testing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:1.5em;'>The weights and subsequently tau and resistance generated are used as the trained model values and tests are run to evaluate the performance. Input characters are presented three times in a row for three spike intervals (300ms total). Observed spikes (fired or not fired) are compared to expected values and performance is reported. \n",
    "<a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#intitializeTrainedModelParameters'>intitializeTrainedModelParameters</a>, <a target=\"_blank\" href='/notebooks/furtherFormulas/testingProcesses.ipynb#evaluateClassifierPerf'>evaluateClassifierPerf</a>, <a target=\"_blank\" href='/github/tartavull/snn-rl/blob/master/furtherFormulas/testingProcesses.ipynb#OutputEvaluationResults'>OutputEvaluationResults</a><br><br><center>Further descriptions of the simulation results are <a target=\"_blank\" href='http://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/analysisFurtherFormulas.ipynb'>here</a></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:2.0em;text-decoration:underline;font-weight:bold'>Main code for the simulation:<a id='mainCode'></a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(precisionPerc)? (<ipython-input-4-eb0f549a19c0>, line 271)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-eb0f549a19c0>\"\u001b[0;36m, line \u001b[0;32m271\u001b[0m\n\u001b[0;31m    if evaluateClassifier: print precisionPerc\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(precisionPerc)?\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\tCopyright 2015, Nate Sutton and Ignacio Tartavull\n",
    "\tThis is the main file for the Spiking Neual Networks\n",
    "\tReinforcement Learning simulation.  \n",
    "\n",
    "\tMore info:\n",
    "\thttps://github.com/tartavull/snn-rl/blob/master/README.md\n",
    "\thttp://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/notebooks/introduction.ipynb\n",
    "\thttp://nbviewer.ipython.org/github/tartavull/snn-rl/blob/master/FFSSN.ipynb\n",
    "'''\n",
    "\n",
    "from furtherFormulas.architecture_further_formulas import *\n",
    "from furtherFormulas.cofactorCalculations import *\n",
    "from furtherFormulas.timeAndRefracCalcs import *\n",
    "from furtherFormulas.outputPrinting import *\n",
    "from furtherFormulas.testingProcesses import *\n",
    "from furtherFormulas.lateralInhibition import *\n",
    "from furtherFormulas.generalUtilities import *\n",
    "\n",
    "timeAndRefrac = timeAndRefrac\t\n",
    "\n",
    "class gupta_paper:\n",
    "\n",
    "\t'''\n",
    "\t\tMain program variables are set the simulation is run.  Specific neuron models are\n",
    "\t\tdefined for computing processing of electrophysiology in the soma, direct to soma\n",
    "\t\tsignals, and active dendrites.  Equations with variables\n",
    "\t\tare defined in the equations() objects.\n",
    "\t'''\n",
    "\tneuralnet = Network()\n",
    "\tdictionary = dictionary()\n",
    "\tspiketimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, testingSpikesPerChar, testingEpochs)\n",
    "\ttrainingSpikeTimes = dictionary.spikeTimes(dictionaryLongitude, spikeInterval, trainingSpikesPerChar, trainingEpochs)\n",
    "\tLIK = SpikeGeneratorGroup(N=15, indices=spiketimes[:,0], times=spiketimes[:,1]*ms)\n",
    "\t# W = W and other lines below are to avoid an odd 'reference before assignment' error\n",
    "\tW = W\n",
    "\ttauM = tauM\n",
    "\tneuronIndex = neuronIndex\n",
    "\tgeneralClockDt = generalClockDt\n",
    "\trunTime = runTime\n",
    "\trunTimeScaling = runTimeScaling\n",
    "\tevaluateClassifier = evaluateClassifier\n",
    "\taccelerateTraining = accelerateTraining\n",
    "\tdiracScaling = diracScaling\n",
    "\tsomaDirectScaling = somaDirectScaling\n",
    "\tnegativeWeightReinforcement = negativeWeightReinforcement\n",
    "\tpositiveWeightReinforcement = positiveWeightReinforcement\n",
    "\ttimeAndRefrac = timeAndRefrac\t\n",
    "\ttestRun = testRun\n",
    "\tlatInhibSettings = latInhibSettings\n",
    "\tstandardPrint = standardPrint\n",
    "\tverbosePrint = verbosePrint\n",
    "\ttestingRunTime = testingRunTime\n",
    "\toptResultsFile = optResultsFile\n",
    "\tminWeightsRand = minWeightsRand\n",
    "\tmaxWeightsRand = maxWeightsRand\n",
    "\ttotalSpikeIntervals = totalSpikeIntervals\n",
    "\n",
    "\tdef run_model(self):\n",
    "\t\tneuralnet = self.neuralnet\n",
    "\t\tdictionary = self.dictionary\n",
    "\n",
    "\t\teqs = Equations('''\n",
    "\t\t\tdv/dt = v/(1*second): volt\n",
    "\t\t\tdprelimV/dt = (-prelimV+((Rm/mV)*(SynI+DendI*1.0)))/(tauM) : volt (unless refractory)\n",
    "\t\t\tRm = 80*mV : volt\n",
    "\t\t\ttauM = 30*ms : second\n",
    "\t        V : volt\n",
    "\t        DendI : volt\n",
    "\t        SynI : volt\n",
    "\t        v2 : volt\t\n",
    "\t\t\tUmSpikeFired : volt\t\n",
    "\t\t\tbeginRefrac : volt\n",
    "\t\t    ''')\t\t\t\n",
    "\n",
    "\t\tdendriteEqs = Equations('''\n",
    "\t\t\tdv/dt = (((-v/mV)+((r/mV)*(w/volt)*(dirac/volt)))/(tau))*mV : volt\n",
    "\t\t\tV : volt\n",
    "\t        r : volt\n",
    "\t        w : volt\n",
    "\t        dirac : volt\n",
    "\t        tau : second\n",
    "\t        v2: volt\n",
    "\t\t\t''')\n",
    "\n",
    "\t\tdirectToSomaEqs = Equations('''\n",
    "\t\t\tdv/dt = (((-v/mV)+(summedWandDirac/volt))/(tauS))*mV : volt\n",
    "\t\t\ttauS = 2*ms : second\n",
    "\t\t\tV : volt\n",
    "\t\t\tsummedWandDirac : volt\n",
    "\t\t\tv2 : volt\n",
    "\t\t\tspikeFired : boolean\n",
    "\t\t\t''')\t\t\n",
    "\n",
    "\t\tclass ADDSNeuronModel(NeuronGroup, gupta_paper): \n",
    "\t\t\t'''\n",
    "\t\t\t\tThis is the model used for electrophysiology occuring in the Soma\n",
    "\t\t\t'''\n",
    "\t\t\tneuronIndex = self.neuronIndex\n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\n",
    "\t\t\tdef __init__(self, params):\n",
    "\t\t\t\tself = parseArgs(self, sys.argv, dictionaryLongitude)\t\t\n",
    "\n",
    "\t\t\t\tNeuronGroup.__init__(self, N=dictionaryLongitude, model=eqs,threshold='v>10*mV', reset='v=-0.002 * mV; dv=0; v2=10*mV;UmSpikeFired=1*mV;beginRefrac=1*mV;inhibitionVoltage=prelimV',refractory=8*ms,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tneuronIndex = self.neuronIndex\n",
    "\t\t\t\t\ttimeAndRefrac.spikeIntervalCounter = (floor(timeAndRefrac.time/timeAndRefrac.spikeIntervalUnformatted) * timeAndRefrac.spikeIntervalUnformatted)*10\n",
    "\n",
    "\t\t\t\t\tdef dendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive):\n",
    "\t\t\t\t\t\t'''\n",
    "\t\t\t\t\t\t\tBelow sequentially Dirac, Tau, then Resistance are calculated every end of a spike-time interval.\n",
    "\t\t\t\t\t\t\tThe resulting Dend I is added to the Um calc for the ADDS soma.\n",
    "\t\t\t\t\t\t'''\n",
    "\t\t\t\t\t\ttimeAndRefrac = self.timeAndRefrac\n",
    "\n",
    "\t\t\t\t\t\t# Dirac\n",
    "\t\t\t\t\t\tdendObj[neuronIndex].dirac = diracCalc(dendObj, neuronIndex, spiketimes, timeAndRefrac.time, timeAndRefrac.lastSpikeInterval)\n",
    "\n",
    "\t\t\t\t\t\t# Initialize weights\n",
    "\t\t\t\t\t\tif (evaluationActive==False and timeAndRefrac.time == 0.000):\n",
    "\t\t\t\t\t\t\tdend[neuronIndex].w = W[neuronIndex]*volt\n",
    "\n",
    "\t\t\t\t\t\tif (evaluationActive==False and timeAndRefrac.refractoryPointSwitch==True):\n",
    "\t\t\t\t\t\t\t# Only change weights of neuron fired\n",
    "\t\t\t\t\t\t\tif ADDSObj.UmSpikeFired[neuronIndex] == 1*mV:\n",
    "\t\t\t\t\t\t\t\t# Weights\n",
    "\t\t\t\t\t\t\t\tWeightChangeCalculation(neuronIndex, spiketimes, timeAndRefrac.time, self.negativeWeightReinforcement, self.positiveWeightReinforcement, M, dendObj)\t\n",
    "\t\t\t\t\t\t\t# Tau\n",
    "\t\t\t\t\t\t\ttauDCalc(neuronIndex, dendObj)\n",
    "\t\t\t\t\t\t\t# Resistance\n",
    "\t\t\t\t\t\t\tresistanceCalc(neuronIndex, dendObj, self.tauM)\n",
    "\n",
    "\t\t\t\t\t\t# TODO: check do I need additional loop below?\n",
    "\t\t\t\t\t\tfor indexOfDend in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\tADDSObj.DendI[indexOfDend] = sum(dendObj[indexOfDend].v[:])*dendCalcScaling\n",
    "\n",
    "\t\t\t\t\t\t#print 'ADDSObj.t',ADDSObj.t,'ADDSObj.DendI',ADDSObj.DendI,'neuronIndex',neuronIndex,'dendObj[neuronIndex].dirac',dendObj[neuronIndex].dirac,'dendObj[neuronIndex].tau',dendObj[neuronIndex].tau,'dendObj[neuronIndex].w',dendObj[neuronIndex].w,'dendObj[neuronIndex].r',dendObj[neuronIndex].r\n",
    "\n",
    "\t\t\t\t\tdef somaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj):\n",
    "\t\t\t\t\t\tdotProductWandDirac =  sum(w*d for w,d in zip(dendObj[neuronIndex].w[:], dendObj[neuronIndex].dirac[:]))\n",
    "\t\t\t\t\t\t#somaDirectObj.summedWandDirac[neuronIndex] = ((dotProductWandDirac*volt)/(volt**2))*self.somaDirectScaling\n",
    "\t\t\t\t\t\tsomaDirect.summedWandDirac[neuronIndex] = ((dotProductWandDirac*volt)/(volt**2))*self.somaDirectScaling\n",
    "\n",
    "\t\t\t\t\t\tfor neuronNumber in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t#ADDSObj.SynI[neuronNumber] = somaDirectObj.v[neuronNumber]\t\t\n",
    "\t\t\t\t\t\t\tADDS.SynI[neuronNumber] = somaDirect.v[neuronNumber]\t\t\n",
    "\n",
    "\t\t\t\t\t\t#print 'ADDSObj.t',ADDSObj.t,'ADDSObj.SynI',ADDSObj.SynI,'neuronIndex',neuronIndex,'somaDirectObj.summedWandDirac',somaDirectObj.summedWandDirac[neuronIndex],'dendObj[neuronIndex].w',dendObj[neuronIndex].w,'dendObj[neuronIndex].dirac',dendObj[neuronIndex].dirac\n",
    "\n",
    "\t\t\t\t\tdef mainSimulationCalcs(ADDSObj, dendObj, somaDirectObj, spiketimes, evaluationActive):\n",
    "\t\t\t\t\t\t'''\n",
    "\t\t\t\t\t\t\tdend then somaDirect calcs are done which are then used to set lat inhib.\n",
    "\t\t\t\t\t\t\tSoma Um calcs are done automatically using equations entered for brian\n",
    "\t\t\t\t\t\t\tonce dend and somaDirect are updated\n",
    "\t\t\t\t\t\t'''\n",
    "\t\t\t\t\t\tpreTNorm = self.timeAndRefrac.time\n",
    "\t\t\t\t\t\ttNorm = preTNorm - (floor((preTNorm/.001)*.01) * .1)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tself.timeAndRefrac = timePeriodAndRefractoryCalcs(self.timeAndRefrac)\n",
    "\n",
    "\t\t\t\t\t\tif (evaluationActive==True) and (timeAndRefrac.time == 0.000 or timeAndRefrac.time == 0.001):\n",
    "\t\t\t\t\t\t\tinitializeTrainedModelParameters(dendObj)\n",
    "\n",
    "\t\t\t\t\t\t# Option to accelerate computations for training\n",
    "\t\t\t\t\t\tif self.accelerateTraining == False or (evaluationActive == False and (tNorm <= .005 or tNorm >= .096)):\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\tif self.accelerateTraining == True and (tNorm >= .096 and tNorm < .099):\n",
    "\t\t\t\t\t\t\t\tfor i in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.DendI[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.SynI[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.prelimV[i]=0*mV\n",
    "\t\t\t\t\t\t\t\t\tADDSObj.v[i]=0*mV\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\tfor i2 in range(len(dend[0].v)):\n",
    "\t\t\t\t\t\t\t\t\t\tdendObj[i].v[i2] = 0*mV\n",
    "\t\t\t\t\t\t\t\t\tsomaDirectObj.v[i] = 0*mV\t\t\n",
    "\n",
    "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\tdendCalcs(neuronIndex, ADDSObj, dendObj, spiketimes, evaluationActive)\n",
    "\n",
    "\t\t\t\t\t\t\t\tsomaDirectCalcs(neuronIndex, ADDSObj, somaDirectObj, dendObj)\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\tADDSObj.v, self.latInhibSettings = lateralInhibition(ADDSObj, self.timeAndRefrac, self.latInhibSettings)\n",
    "\n",
    "\t\t\t\t\t\t\t#if ADDSObj.t > 100*ms:\n",
    "\t\t\t\t\t\t\t\t#for i in range(dictionaryLongitude):\n",
    "\t\t\t\t\t\t\t\t#\tsomaDirectObj.summedWandDirac[i] += 20*mV\n",
    "\t\t\t\t\t\t\t\t#print 'ADDS.t',ADDS.t,'ADDS.SynI',ADDS.SynI,'ADDS.DendI',ADDS.DendI,'ADDSObj.v',ADDSObj.v,'somaDirectObj.summedWandDirac',somaDirectObj.summedWandDirac,'somaDirectObj.v',somaDirectObj.v\n",
    "\n",
    "\t\t\t\t\t\t\tfor neuronIndex in range(dictionaryLongitude): \n",
    "\t\t\t\t\t\t\t\tADDSObj.v2, somaDirectObj.v2, self.timeAndRefrac = checkForResets(neuronIndex, ADDSObj, dendObj, somaDirectObj, self.timeAndRefrac)\n",
    "\n",
    "\t\t\t\t\t\t\tADDSObj.UmSpikeFired, self.testRun = evaluateClassifierPerf2(ADDSObj, self.testRun)\n",
    "\n",
    "\t\t\t\t\t\t#roundedSecondsTime = math.floor(Decimal(format((ADDSObj.t), '.1f'))/Decimal(format((1.0*second), '.1f')))\n",
    "\t\t\t\t\t\t#if printWeights < roundedSecondsTime:\n",
    "\t\t\t\t\t\t#\tself.printWeights = roundedSecondsTime; printWeights(dendObj);\n",
    "\t\t\t\t\tif self.evaluateClassifier == False:\n",
    "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.trainingSpikeTimes, False)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tmainSimulationCalcs(ADDS, dend, somaDirect, self.spiketimes, True)\t\t\t\t\t\n",
    "\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\t\t\n",
    "\n",
    "\t\tclass DendriteNeuronModel(NeuronGroup):\n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\t\t\tdef __init__(self, params): \n",
    "\t\t\t\tNeuronGroup.__init__(self, N=15, model=dendriteEqs,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tplaceHolderForLaterContent = True\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\n",
    "\n",
    "\t\tclass SomaDirectNeuronModel(NeuronGroup): \n",
    "\t\t\tgeneralClockDt = self.generalClockDt\n",
    "\t\t\tdef __init__(self, params): \n",
    "\t\t\t\tNeuronGroup.__init__(self, N=dictionaryLongitude, model=directToSomaEqs,clock=Clock(dt=self.generalClockDt))\n",
    "\t\t\t\t@network_operation(dt=self.generalClockDt)\n",
    "\t\t\t\tdef additionToNetwork(): \n",
    "\t\t\t\t\tplaceHolderForLaterContent = True\n",
    "\t\t\t\tself.contained_objects.append(additionToNetwork)\t\t\n",
    "\n",
    "\t\tdend = [None]*dictionaryLongitude\n",
    "\t\tweightMonitors = [None]*dictionaryLongitude\n",
    "\t\tfor firstLayerIndex in range(dictionaryLongitude):\n",
    "\t\t\tdend[firstLayerIndex] = DendriteNeuronModel(15)\t\n",
    "\t\t\tweightMonitors[firstLayerIndex] = StateMonitor(dend[firstLayerIndex], 'w', record=True)\n",
    "\t\t\tneuralnet.add(dend[firstLayerIndex])\n",
    "\t\t\tneuralnet.add(weightMonitors[firstLayerIndex])\n",
    "\t\tsomaDirect = SomaDirectNeuronModel(dictionaryLongitude)\n",
    "\t\tneuralnet.add(somaDirect)\n",
    "\t\tADDS = ADDSNeuronModel(self)\t\t\t\n",
    "\t\tM = SpikeMonitor(ADDS)\n",
    "\t\tMv = StateMonitor(ADDS, 'V', record=True)\n",
    "\t\tUmM = StateMonitor(ADDS, 'v2', record=True)\n",
    "\t\tself.M = M # for ipython compatibility\n",
    "\t\tself.UmM = UmM \n",
    "\t\tself.weightMonitors = weightMonitors\n",
    "\n",
    "\t\tneuralnet.add(ADDS)\n",
    "\t\tneuralnet.add(M)\n",
    "\t\tneuralnet.add(UmM)\n",
    "\t\tif (ADDS.evaluateClassifier==True): ADDS.runTime = ADDS.testingRunTime\n",
    "\t\tADDS.runTime *= ADDS.runTimeScaling # scaling factor\n",
    "\t\tif ADDS.standardPrint: neuralnet.run(ADDS.runTime,report='text')\n",
    "\t\telse: neuralnet.run(ADDS.runTime,report='stderr')\n",
    "\n",
    "\t\tOutputEvaluationResults(dend, self.testRun, ADDS.verbosePrint, ADDS.evaluateClassifier)\n",
    "\t\taccuracyPerc = totalCorrectPercentage()\n",
    "\t\tprecisionPerc = precisionPercentage()\n",
    "\t\twriteOptimizationResults(ADDS, self.testRun, accuracyPerc)\n",
    "\n",
    "\t\tneuronToPlot = 1\n",
    "\t\tcolors = ['r']*1+['g']*1+['b']*1+['y']*1\n",
    "\t\tcolors = ['blue', 'green', 'magenta', 'cyan']\n",
    "\t\tsubplot(211)\n",
    "\t\tplot(M.t/ms, M.i, '.')\n",
    "\t\tlegend(['A','B','C','D'], loc='upper left')\t\t\t\n",
    "\t\tsubplot(212)\n",
    "\t\tplot(UmM.t, UmM.v2.T/mV)\t\n",
    "\t\txlabel('Time (ms)')\n",
    "\t\tylabel('Membrane Potential (mV)')\n",
    "\t\tif (showPlot==True):\n",
    "\t\t\tshow()\t\n",
    "\n",
    "\t\treturn evaluateClassifier, precisionPerc\n",
    "\n",
    "def main():\n",
    "\trun_gupta_paper = gupta_paper()\n",
    "\tevaluateClassifier, precisionPerc = run_gupta_paper.run_model()\n",
    "\tif evaluateClassifier: print precisionPerc\n",
    "\treturn precisionPerc\n",
    "\n",
    "if  __name__ =='__main__':main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
